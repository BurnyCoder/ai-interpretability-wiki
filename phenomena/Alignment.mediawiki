<onlyinclude>
'''[[Alignment]]''', or '''AI alignment''', is "ensuring AI systems pursue intended goals"<ref name="bereska-review">[[Mechanistic Interpretability for AI Safety -- A Review]].</ref> "with human values."<ref name="bereska-review" /> "Understanding AI systems' inner workings is critical for ensuring value alignment and safety."<ref name="bereska-review" />
</onlyinclude>

== References ==

<references />

[[Category:Phenomena]]

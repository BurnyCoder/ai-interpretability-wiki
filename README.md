# AI Interpretability Wiki

A local mirror of the [AI Interpretability Wiki](https://aiinterpretability.miraheze.org/), written in MediaWiki and hosted on Miraheze.

## About

**Interpretability** is the ability for the decision processes and inner workings of artificial intelligence and machine learning systems to be understood by humans or other outside observers. [[source](https://www.lesswrong.com/w/interpretability-ml-and-ai)]

**Mechanistic interpretability** (often abbreviated as **mech interp**, **mechinterp**, or **MI**) is a subfield of research within AI interpretability and explainable artificial intelligence that aims to understand the internal workings of neural networks by analyzing the mechanisms present in their computations. The approach seeks to analyze neural networks in a manner similar to how binary computer programs can be reverse-engineered to understand their functions. [[source](https://en.wikipedia.org/wiki/Mechanistic_interpretability)] "This can be contrasted to subfieds of interpretability which seek to attribute some output to a part of a specific input, such as clarifying which pixels in an input image caused a computer vision model to output the classification 'horse'." [[source](https://www.lesswrong.com/w/interpretability-ml-and-ai)]

## Contents

| Category | Description |
|----------|-------------|
| Highlighted work | Notable and influential work in AI interpretability |
| Papers | Papers in AI interpretability |
| Concepts | Core ideas and terminology |
| Methods | Techniques and approaches used to interpret models |
| Architectures for interpretability | Architectures used to interpret models |
| Applications | Practical uses and deployments |
| Phenomena | Behaviors and properties analyzed |
| Theory | Theoretical foundations, mathematical and formal frameworks in AI and AI interpretability |
| AI architectures | Architectures of studied models |
| Interpretability architectures | Architectures that are designed to be more interpretable |
| Surveys | Survey papers and literature reviews |
| Github codebases | Code repositories |
| People and groups | Researchers, labs, and organizations |
| Communities | Communities on Discord, Slack, etc. |
| Feeds | Feeds for papers, blogs, newsletters, and content feeds |
| Youtube channels | Video content and educational channels |
| Resources for learning | Tutorials, courses, and learning materials |
| Events | Conferences, workshops, and meetups |

## All pages

### Highlighted work

* Progress measures for grokking via mechanistic interpretability
* Towards Monosemanticity: Decomposing Language Models With Dictionary Learning
* On the Biology of a Large Language Model
* When Models Manipulate Manifolds: The Geometry of a Counting Task

### Papers

* Progress measures for grokking via mechanistic interpretability
* Towards Monosemanticity: Decomposing Language Models With Dictionary Learning
* On the Biology of a Large Language Model
* When Models Manipulate Manifolds: The Geometry of a Counting Task

### Concepts

* Feature
* Circuit
* Pragmatic interpretability
* Explainable AI

### Methods

* Dictionary learning

### Architectures for interpretability

* Sparse autoencoder
* Linear probe
* Cross-layer transcoder

### AI architectures

* Transformer
* CNN

### Phenomena

* Alignment
* Grokking
* Jailbreak
* Addition
* Curve detector
* Dog detector
* Golden Gate Bridge Claude

### Theory

* A Mathematical Framework for Transformer Circuits
* The Principles of Deep Learning Theory
* Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges

### Surveys

* Mechanistic Interpretability for AI Safety -- A Review
* Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models
* An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers v2

### Github codebases

* TransformerLens

### People and groups

* Neel Nanda

### Communities

* Mech Interp discord

### Feeds

* Anthropic Interpretability research
* Transformer Circuits Thread

### Resources for learning

* ARENA
* How to become a mechanistic interpretability researcher

## Tips

* To check which pages link to the page you're on, go to the Tools menu on the top right and click on "What links here".
* To enable dark mode, log in, go to Preferences, Appearance, and select Dark. Alternatively, use the [Dark Reader](https://chromewebstore.google.com/detail/dark-reader/eimadpbcbfnmbkopoojfekhnkhdbieeh) Chrome extension, but it might be a bit broken.

## Random page

[Click here to go to a random page on the wiki!](https://aiinterpretability.miraheze.org/wiki/Special:Random)

## Wiki created by

* Burny ([website](https://burnyverse.com/), [X](https://x.com/burny_tech), [GitHub](https://github.com/BurnyCoder), [LinkedIn](https://www.linkedin.com/in/libor-burian-1113b0207/), [Facebook](https://www.facebook.com/burian.libor/), [Telegram](https://t.me/burnytech), [BlueSky](https://burnytech.bsky.social), [Mastodon](https://mathstodon.xyz/@Burny), Discord: burnytech, [burian.lib@gmail.com](mailto:burian.lib@gmail.com)).

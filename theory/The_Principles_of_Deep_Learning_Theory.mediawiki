<onlyinclude>
'''[[The Principles of Deep Learning Theory]]''' "develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks," the authors "explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics."<ref name="principles">Roberts, Daniel A.; Yaida, Sho; Hanin, Boris (2021). [https://arxiv.org/abs/2106.10165 "The Principles of Deep Learning Theory"]. arXiv:[https://arxiv.org/abs/2106.10165 2106.10165].</ref> "A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description."<ref name="principles" /> The book develops "the notion of representation group flow (RG flow) to characterize the propagation of signals through the network," gives "a practical solution to the exploding and vanishing gradient problem" by "tuning networks to criticality," and shows that "the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks."<ref name="principles" />
</onlyinclude>

== References ==

<references />

[[Category:Theory]]

== Welcome to {{SITENAME}} wiki! ==

Present-day [https://en.wikipedia.org/wiki/Artificial_intelligence artificial intelligence] and [https://en.wikipedia.org/wiki/Machine_learning machine learning] systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models.<ref name="lesswrong-interp">[https://www.lesswrong.com/w/interpretability-ml-and-ai "Interpretability (ML & AI)"]. ''LessWrong''.</ref>

'''Interpretability''' is the broader subfield of AI studying why AI systems do what they do, and trying to put this into human-understandable terms.<ref name="nanda-glossary">[https://www.neelnanda.io/mechanistic-interpretability/glossary "Mechanistic Interpretability Glossary"]. ''Neel Nanda''.</ref> It is the ability for the decision processes and inner workings of artificial intelligence and machine learning systems to be understood by humans or other outside observers.<ref name="lesswrong-interp" />

'''Mechanistic interpretability''' (often abbreviated as '''mech interp''', '''mechinterp''', or '''MI''') is a prominent subfield of interpretability and the field of study of reverse engineering neural networks from the learned weights down to human-interpretable algorithms.<ref name="nanda-glossary" /> It attempts to understand how neural networks perform the tasks they perform by analyzing the mechanisms present in their computations, for example by finding circuits in transformer models.<ref>[https://en.wikipedia.org/wiki/Mechanistic_interpretability "Mechanistic interpretability"]. ''Wikipedia''.</ref> The approach is analogous to reverse engineering a compiled program binary back to source code.<ref name="nanda-glossary" /> "Mechanistic" refers to the emphasis on trying to understand the actual mechanisms and algorithms that compose the network.<ref name="nanda-glossary" /> In contrast, many forms of interpretability seek to explain how the network's outputs relate high level concepts without referencing the actual functioning of the network. [https://christophm.github.io/interpretable-ml-book/pixel-attribution.html Saliency maps] are a classic example, as are "build an interpretable model" techniques such as [https://homes.cs.washington.edu/~marcotcr/blog/lime/ LIME].<ref name="nanda-glossary" />

== Further reading ==

* [https://distill.pub/2020/circuits/zoom-in/ "Circuits: Zoom In"]. ''Distill''.
* [https://transformer-circuits.pub/2022/mech-interp-essay/index.html "Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases"]. ''Transformer Circuits''.
* [http://colah.github.io/notes/interp-v-neuro/ "Interpretability vs Neuroscience"]. ''colah's blog''.
* [http://colah.github.io/notes/bio-analogies/ "Analogies between biology and deep learning"]. ''colah's blog''.
* [https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=bsKRuga24AHSMztSeter7oGX "Discussion of the relationship to the rest of the field"]. ''Dynalist''.

== Contents ==

{| class="wikitable" style="width:100%;"
|-
! Category !! Description
|-
| [[:Category:Highlighted work|Highlighted work]] || Notable and influential work in AI interpretability
|-
| [[:Category:Concepts|Concepts]] || Core ideas and terminology
|-
| [[:Category:Methods|Methods]] || Techniques and approaches used to interpret models
|-
| [[:Category:Architectures for interpretability|Architectures for interpretability]] || Architectures used to interpret models
|-
| [[:Category:Applications|Applications]] || Practical uses and deployments
|-
| [[:Category:Phenomena|Phenomena]] || Behaviors and properties analyzed
|-
| [[:Category:Theory|Theory]] || Theoretical foundations, mathematical and formal frameworks in AI and AI interpretability
|-
| [[:Category:AI architectures|AI architectures]] || Architectures of studied models 
|-
| [[:Category:Interpretable architectures|Interpretability architectures]] || Architectures that are designed to be more interpretable
|-
| [[:Category:Surveys|Surveys]] || Survey papers and literature reviews
|-
| [[:Category:Github codebases|Github codebases]] || Code repositories
|-
| [[:Category:People and groups|People and groups]] || Researchers, labs, and organizations 
|-
| [[:Category:Communities|Communities]] || Communities on Discord, Slack, etc.
|-
| [[:Category:Feeds|Feeds]] || Blogs, newsletters, and content feeds
|-
| [[:Category:Youtube channels|Youtube channels]] || Video content and educational channels
|-
| [[:Category:Resources for learning|Resources for learning]] || Tutorials, courses, and learning materials
|-
| [[:Category:Events|Events]] || Conferences, workshops, and meetups
|}

== References ==

<references />

== Created by == 

* Burny ([https://burnyverse.com/ website], [https://x.com/burny_tech X], [https://github.com/BurnyCoder GitHub], [https://www.linkedin.com/in/libor-burian-1113b0207/ LinkedIn], [https://www.facebook.com/burian.libor/ Facebook], [https://t.me/burnytech Telegram], [https://burnytech.bsky.social BlueSky], [https://mathstodon.xyz/@Burny Mastodon], [https://patreon.com/BurnyTech Patreon], Discord: burnytech, [mailto:burian.lib@gmail.com burian.lib@gmail.com]).

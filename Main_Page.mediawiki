== Welcome to {{SITENAME}} wiki! ==

"Present-day [https://en.wikipedia.org/wiki/Artificial_intelligence artificial intelligence] and [https://en.wikipedia.org/wiki/Machine_learning machine learning] systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models."<ref name="lesswrong-interp">[https://www.lesswrong.com/w/interpretability-ml-and-ai "Interpretability (ML & AI)"]. ''LessWrong''.</ref>

'''Interpretability''' is "the broader subfield of AI studying why AI systems do what they do, and trying to put this into human-understandable terms."<ref name="nanda-glossary">[https://www.neelnanda.io/mechanistic-interpretability/glossary "Mechanistic Interpretability Glossary"]. ''Neel Nanda''.</ref> It is "the ability for the decision processes and inner workings of artificial intelligence and machine learning systems to be understood by humans or other outside observers."<ref name="lesswrong-interp" />

'''Mechanistic interpretability''' (often abbreviated as '''mech interp''', '''mechinterp''', or '''MI''') is a prominent subfield of interpretability within explainable artificial intelligence and the field of study that attempts to reverse engineer neural networks from the learned weights down to human-interpretable algorithms where it's possible. It attempts to understand how neural networks perform the tasks they perform by analyzing which mechanisms are present in their computations when they are present, for example by finding interpretable [[feature|features]] and [[circuit|circuits]] in transformer models using [[Sparse autoencoder|sparse autoencoders]] and [[Attribution graph|attribution graphs]], such as how models add two numbers together.<ref name="nanda-glossary" /><ref>[https://en.wikipedia.org/wiki/Mechanistic_interpretability "Mechanistic interpretability"]. ''Wikipedia''.</ref><ref>[[On the Biology of a Large Language Model]].</ref> The approach is "analogous to reverse engineering a compiled program binary back to source code."<ref name="nanda-glossary" /> "Mechanistic" refers to the emphasis on trying to understand the actual mechanisms and algorithms that possibly compose the network.<ref name="nanda-glossary" /> "In contrast, many forms of interpretability seek to explain how the network's outputs relate high level concepts without referencing the actual functioning of the network."<ref name="nanda-glossary" /> "[[Saliency map|Saliency maps]] are a classic example,"<ref name="nanda-glossary" /> "seek[ing] to attribute some output to a part of a specific input, such as clarifying which pixels in an input image caused a computer vision model to output the classification 'horse',"<ref name="lesswrong-interp" /> "as are 'build an interpretable model' techniques such as [[LIME]]."<ref name="nanda-glossary" />

== Contents ==

{| class="wikitable" style="width:100%;"
|-
! Category !! Description
|-
| [[:Category:Highlighted work|Highlighted work]] || Notable and influential work in AI interpretability
|-
| [[:Category:Concepts|Concepts]] || Core ideas and terminology
|-
| [[:Category:Methods|Methods]] || Techniques and approaches used to interpret models
|-
| [[:Category:Architectures for interpretability|Architectures for interpretability]] || Architectures used to interpret models
|-
| [[:Category:Applications|Applications]] || Practical uses and deployments
|-
| [[:Category:Phenomena|Phenomena]] || Behaviors and properties analyzed
|-
| [[:Category:Theory|Theory]] || Theoretical foundations, mathematical and formal frameworks in AI and AI interpretability
|-
| [[:Category:AI architectures|AI architectures]] || Architectures of studied models 
|-
| [[:Category:Interpretable architectures|Interpretability architectures]] || Architectures that are designed to be more interpretable
|-
| [[:Category:Surveys|Surveys]] || Survey papers and literature reviews
|-
| [[:Category:Github codebases|Github codebases]] || Code repositories
|-
| [[:Category:People and groups|People and groups]] || Researchers, labs, and organizations 
|-
| [[:Category:Communities|Communities]] || Communities on Discord, Slack, etc.
|-
| [[:Category:Feeds|Feeds]] || Blogs, newsletters, and content feeds
|-
| [[:Category:Youtube channels|Youtube channels]] || Video content and educational channels
|-
| [[:Category:Resources for learning|Resources for learning]] || Tutorials, courses, and learning materials
|-
| [[:Category:Events|Events]] || Conferences, workshops, and meetups
|}

== Pages ==

=== [[:Category:Highlighted work|Highlighted work]] ===

* [[Progress measures for grokking via mechanistic interpretability]]
* [[Towards Monosemanticity: Decomposing Language Models With Dictionary Learning]]

=== [[:Category:Concepts|Concepts]] ===

* [[Feature]]

=== [[:Category:Architectures for interpretability|Architectures for interpretability]] ===

* [[Sparse autoencoder]]

=== [[:Category:Phenomena|Phenomena]] ===

* [[Grokking]]

=== [[:Category:People and groups|People and groups]] ===

* [[Neel Nanda]]

== Random page ==

[[Special:Random|Click here to go to a random page on the wiki!]]

== References ==

<references />

== Tips ==

* To learn how to create a new page, see [[How to create a page]].
* To check which pages link to the page you're on, go to the Tools menu on the top right and click on "What links here".
* To enable dark mode, log in, go to Preferences, Appearance, and select Dark. Alternatively, use the [https://chromewebstore.google.com/detail/dark-reader/eimadpbcbfnmbkopoojfekhnkhdbieeh Dark Reader] Chrome extension, but it might be a bit broken.

== Created by == 

* Burny ([https://burnyverse.com/ website], [https://x.com/burny_tech X], [https://github.com/BurnyCoder GitHub], [https://www.linkedin.com/in/libor-burian-1113b0207/ LinkedIn], [https://www.facebook.com/burian.libor/ Facebook], [https://t.me/burnytech Telegram], [https://burnytech.bsky.social BlueSky], [https://mathstodon.xyz/@Burny Mastodon], Discord: burnytech, [mailto:burian.lib@gmail.com burian.lib@gmail.com]).

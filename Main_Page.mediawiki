== Welcome to {{SITENAME}} wiki! ==

Present-day [https://en.wikipedia.org/wiki/Artificial_intelligence artificial intelligence] and [https://en.wikipedia.org/wiki/Machine_learning machine learning] systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models.<ref name="lesswrong-interp">[https://www.lesswrong.com/w/interpretability-ml-and-ai "Interpretability (ML & AI)"]. ''LessWrong''.</ref>

'''Interpretability''' is the ability for the decision processes and inner workings of artificial intelligence and machine learning systems to be understood by humans or other outside observers.<ref name="lesswrong-interp" />

'''Mechanistic interpretability''' (often abbreviated as '''mech interp''', '''mechinterp''', or '''MI''') is a prominent subfield of research within AI interpretability and explainable artificial intelligence that attempts to understand how neural networks perform the tasks they perform by analyzing the mechanisms present in their computations, for example by finding circuits in transformer models. The approach seeks to analyze neural networks in a manner similar to how binary computer programs can be reverse-engineered to understand their functions. This can be contrasted to subfields of interpretability which seek to attribute some output to a part of a specific input, such as clarifying which pixels in an input image caused a computer vision model to output the classification "horse".<ref>[https://en.wikipedia.org/wiki/Mechanistic_interpretability "Mechanistic interpretability"]. ''Wikipedia''.</ref>

== Contents ==

{| class="wikitable" style="width:100%;"
|-
! Category !! Description
|-
| [[:Category:Highlighted work|Highlighted work]] || Notable and influential work in AI interpretability
|-
| [[:Category:Concepts|Concepts]] || Core ideas and terminology
|-
| [[:Category:Methods|Methods]] || Techniques and approaches used to interpret models
|-
| [[:Category:Architectures for interpretability|Architectures for interpretability]] || Architectures used to interpret models
|-
| [[:Category:Applications|Applications]] || Practical uses and deployments
|-
| [[:Category:Phenomena|Phenomena]] || Behaviors and properties analyzed
|-
| [[:Category:Theory|Theory]] || Theoretical foundations, mathematical and formal frameworks in AI and AI interpretability
|-
| [[:Category:AI architectures|AI architectures]] || Architectures of studied models 
|-
| [[:Category:Interpretable architectures|Interpretability architectures]] || Architectures that are designed to be more interpretable
|-
| [[:Category:Surveys|Surveys]] || Survey papers and literature reviews
|-
| [[:Category:Github codebases|Github codebases]] || Code repositories
|-
| [[:Category:People and groups|People and groups]] || Researchers, labs, and organizations 
|-
| [[:Category:Communities|Communities]] || Communities on Discord, Slack, etc.
|-
| [[:Category:Feeds|Feeds]] || Blogs, newsletters, and content feeds
|-
| [[:Category:Youtube channels|Youtube channels]] || Video content and educational channels
|-
| [[:Category:Resources for learning|Resources for learning]] || Tutorials, courses, and learning materials
|-
| [[:Category:Events|Events]] || Conferences, workshops, and meetups
|}

== References ==

<references />

== Created by == 

* Burny ([https://burnyverse.com/ website], [https://x.com/burny_tech X], [https://github.com/BurnyCoder GitHub], [https://www.linkedin.com/in/libor-burian-1113b0207/ LinkedIn], [https://www.facebook.com/burian.libor/ Facebook], [https://t.me/burnytech Telegram], [https://burnytech.bsky.social BlueSky], [https://mathstodon.xyz/@Burny Mastodon], [https://patreon.com/BurnyTech Patreon], Discord: burnytech, [mailto:burian.lib@gmail.com burian.lib@gmail.com]).

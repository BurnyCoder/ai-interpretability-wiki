<onlyinclude>'''[[When Models Manipulate Manifolds: The Geometry of a Counting Task]]''' investigates "the mechanisms that enable Claude 3.5 Haiku to perform a natural perceptual task which is common in pretraining corpora and involves tracking position in a document," using [[Attribution graph|attribution graphs]] with [[Feature|features]] from a [[cross-layer transcoder]] dictionary.<ref name="linebreaks">Gurnee, Wes; Ameisen, Emmanuel; Kauvar, Isaac; et al. (2025). [https://transformer-circuits.pub/2025/linebreaks/index.html "When Models Manipulate Manifolds: The Geometry of a Counting Task"]. ''Transformer Circuits Thread''.</ref> The task studied is "linebreaking in fixed-width text," where "the model must somehow count the characters in the current line, subtract that from the line width constraint of the document, and compare the number of characters remaining to the length of the next word."<ref name="linebreaks" /> The paper finds that these quantities are each "represented on 1-dimensional feature manifolds embedded with high curvature in low-dimensional subspaces of the residual stream," discovers attention heads that detect line boundaries by rotating one counting manifold to align with another, and validates these interpretations through "targeted interventions, ablations, and 'visual illusions' â€” character sequences that hijack specific attention mechanisms to disrupt spatial perception."<ref name="linebreaks" /></onlyinclude>

<ref name="linebreaks" />

== References ==

<references />

[[Category:Highlighted work]]
[[Category:Papers]]

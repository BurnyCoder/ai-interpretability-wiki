<onlyinclude>
* "'''[[Feature|Features]]''' are the fundamental building block of models - the model's internal activations represent features, and the model's weights and non-linearities are used to apply computations to produce later features from earlier features."<ref name="nanda-glossary">Nanda, Neel (2022). [https://www.neelnanda.io/mechanistic-interpretability/glossary "A Comprehensive Mechanistic Interpretability Explainer & Glossary"].</ref> Alternatively, "a feature is a property of an input to the model, or some subset of that input (eg a token in the prompt given to a language model, or a patch of an image)."<ref name="nanda-glossary" /> Alternatively, "features are whatever the 'independent units' a neural network representation can be decomposed into are."<ref name="olah-2022">Olah, Chris (2022-06-27). [https://transformer-circuits.pub/2022/mech-interp-essay/index.html "Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases"]. ''Transformer Circuits Thread''.</ref>
</onlyinclude>

== Other definitions == 

* A feature is something like curve detector or car detector neurons found in InceptionV1 - some meaningful, articulable property of the input which the network encodes as a direction in activation space. The unsatisfactory thing about this definition is that it is human centric: it excludes the possibility of features humans don't understand.<ref name="olah-2022" />
* Features are the things a network would ideally dedicate a neuron to if you gave it enough neurons.<ref name="olah-2022" />
* Some researchers define features simply as any function of the input. The unsatisfactory thing about this is that under this definition, any mixture of features is also a feature. There is something importantly different about a "car detector" (which seems to correspond to an important latent variable) from "car and cat detector" (which seems to be an arbitrary mixture of things).<ref name="olah-2022" />
* In practice, the term is normally used to describe a property of an example input that is actually internally represented in the model, rather than a feature that could be in theory. It refers to properties of inputs that generalise across inputs.<ref name="nanda-glossary" />

== Examples ==

* This part of the image contains a curve
* This is a feature in a convnet, where there's a neuron activation per image patch - thus "part of image"
* This part of the image contains a dog fur-like texture
* This token is the final token in the phrase "Eiffel Tower"
* In a factual recall circuit, this can get looked up to produce the feature "is in Paris"
* This is a feature in a transformer, where there are separate activations for each token in a sequence, thus "this token"
* This text is Python code
* This token is the name of a variable corresponding to a list in Python code
* This token is in a news headline in a Reuters article
* This token corresponds to a number that is being used to describe a group of people
* This text is a Bible verse
* This text is an excerpt from Harry Potter<ref name="nanda-glossary" />
* Curve detector or car detector neurons found in InceptionV1<ref name="olah-2022" />

== References ==

<references />

[[Category:Concepts]]

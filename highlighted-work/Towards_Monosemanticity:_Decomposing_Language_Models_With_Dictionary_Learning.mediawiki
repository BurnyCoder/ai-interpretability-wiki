<onlyinclude>'''[[Towards Monosemanticity: Decomposing Language Models With Dictionary Learning]]''' uses a [[Sparse autoencoder|sparse autoencoder]] to extract a large number of interpretable features from a one-layer transformer, generating learned features that offer a more monosemantic unit of analysis than the model's neurons themselves.<ref>Bricken, Trenton; Templeton, Adly; Batson, Joshua; et al. (2023). [https://transformer-circuits.pub/2023/monosemantic-features "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning"]. ''Transformer Circuits Thread''.</ref><ref>Anthropic (2023). [https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning"]. ''Anthropic Research''.</ref></onlyinclude>

== References ==

<references />

[[Category:Highlighted work]]

<onlyinclude>'''[[Towards Monosemanticity: Decomposing Language Models With Dictionary Learning]]''' is a 2023 paper by Anthropic in the Transformer Circuits Thread.<ref>[https://transformer-circuits.pub/2023/monosemantic-features Towards Monosemanticity: Decomposing Language Models With Dictionary Learning — Transformer Circuits Thread] (paper)</ref><ref>[https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning Towards Monosemanticity: Decomposing Language Models With Dictionary Learning — Anthropic Research] (blog post)</ref> It uses a sparse autoencoder to extract a large number of interpretable features from a one-layer transformer, generating learned features that offer a more monosemantic unit of analysis than the model's neurons themselves.</onlyinclude>

== References ==

<references />

[[Category:Highlighted work]]

<onlyinclude>'''[[Progress measures for grokking via mechanistic interpretability]]''' fully reverse engineers the algorithm learned by small transformers trained on modular addition tasks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle.<ref>Nanda, Neel; Chan, Lawrence; Lieberum, Tom; et al. (2023). [https://arxiv.org/abs/2301.05217 "Progress measures for grokking via mechanistic interpretability"]. ''International Conference on Learning Representations (ICLR 2023)''. arXiv:[https://arxiv.org/abs/2301.05217 2301.05217].</ref> Based on this understanding, they define progress measures that allow them to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Their results show that [[Grokking|grokking]], rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.</onlyinclude>

It is a 2023 paper by Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt, published as a conference paper at ICLR 2023.

== References ==

<references />

[[Category:Highlighted work]]

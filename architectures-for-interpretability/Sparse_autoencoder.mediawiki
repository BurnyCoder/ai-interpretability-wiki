<onlyinclude>
'''[[Sparse autoencoder]]''' ('''SAE''') is "a weak dictionary learning algorithm to generate learned [[Feature|features]] from a trained model that offer a more monosemantic unit of analysis than the model's neurons themselves."<ref name="towards-monosemanticity">Bricken, Trenton; Templeton, Adly; Batson, Joshua; et al. (2023). [https://transformer-circuits.pub/2023/monosemantic-features/index.html "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning"]. ''Transformer Circuits Thread''.</ref>
</onlyinclude>

Sparse autoencoders address the problem of polysemanticity, where "many neurons are polysemantic: they respond to mixtures of seemingly unrelated inputs."<ref name="towards-monosemanticity" /> "Polysemanticity makes it difficult to reason about the behavior of the network in terms of the activity of individual neurons."<ref name="towards-monosemanticity" /> One potential cause of polysemanticity is superposition, "a hypothesized phenomenon where a neural network represents more independent 'features' of the data than it has neurons by assigning each feature its own linear combination of neurons."<ref name="towards-monosemanticity" /> Sparse autoencoders help resolve this, as they "extract relatively monosemantic features" and "produce interpretable features that are effectively invisible in the neuron basis."<ref name="towards-monosemanticity" />

== References ==

<references />

[[Category:Architectures for interpretability]]
